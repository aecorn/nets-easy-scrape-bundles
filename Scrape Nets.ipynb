{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppstart\n",
    "\n",
    "Sjekk hvilken chrome du bruker under help\n",
    "\n",
    "Last ned chromedriver\n",
    "https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "Legg på /Users/User/Documents/WebDriver\n",
    "\n",
    "Husk å sette path til chromedriver.\n",
    "1. Open up Terminal.\n",
    "1. Run sudo nano /etc/paths.\n",
    "1. Enter your password.\n",
    "1. Go to the bottom of the file and enter the path you wish to add.\n",
    "1. My PATH looks like: /usr/local/bin\n",
    "1. Control-x to quit.\n",
    "1. Y to save.\n",
    "1. Press enter to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as re\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bruker = widgets.Text(\n",
    "    value='Brukernavn',\n",
    "    placeholder='Mailadresse hos nets',\n",
    "    description='Mail:',\n",
    "    disabled=False\n",
    ")\n",
    "passwrd = widgets.Text(\n",
    "    value='Passord',\n",
    "    placeholder='Passord hos nets',\n",
    "    description='Passord:',\n",
    "    disabled=False\n",
    ")\n",
    "display(bruker, passwrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bruker.value)\n",
    "print(passwrd.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"Documents/Nets_Scrape/\"\n",
    "output_csv = \"1desember-4jan_side1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_file = \"Easyadministration.html\"\n",
    "url = 'https://portal.dibspayment.eu/payouts'\n",
    "#page = re.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_path = '/usr/local/bin/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=chrome_path)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login om nødvendig\n",
    "driver.find_element_by_id('loginEmail').send_keys(bruker.value)\n",
    "driver.find_element_by_id('loginPassword').send_keys(passwrd.value)\n",
    "driver.find_element_by_css_selector('button#login').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# START OVER FROM HERE AFTER NAVIGATING TO WANTED DATE RANGE #\n",
    "##################################################################\n",
    "\n",
    "soup = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(soup, 'html.parser')\n",
    "main_trs = soup.find('table', class_='table-striped').find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_subpage(soup):\n",
    "    main_trs = soup.find('table', class_='table-striped').find_all('tr')\n",
    "    \n",
    "    header = []\n",
    "    main_list = []\n",
    "    count = 0\n",
    "\n",
    "    hr_cells = main_trs[0].find_all('th')\n",
    "    for cell in hr_cells:\n",
    "        header.append(cell.text.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace('NOK', ''))\n",
    "\n",
    "    for row in main_trs:\n",
    "        under_main_list = []\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            for cell in cells:\n",
    "                under_main_list.append(cell.text.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\"))\n",
    "            main_list.append(under_main_list)\n",
    "            \n",
    "    main_temp = [x for x in main_list[:-1]]\n",
    "\n",
    "    main_trans = soup.find('div', class_='block-hero')\n",
    "\n",
    "    main_trans_head = []\n",
    "    main_trans_cont=  []\n",
    "\n",
    "    referanse = soup.find('span', class_='payout-referene').text.strip()\n",
    "    main_trans_head.append(referanse.split(\": \")[0])\n",
    "    main_trans_cont.append(referanse.split(\": \")[1])\n",
    "\n",
    "    main_trans_head.append(main_trans.find('p', class_='payout-date').text.strip().split(\"\\n\")[0])\n",
    "    main_trans_cont.append(main_trans.find('span', class_='date-cell').text.strip())\n",
    "\n",
    "    mntrns_maintb = main_trans.find('table', class_='order-summary-list').find_all('tr')\n",
    "    count = 0\n",
    "    for row in mntrns_maintb:\n",
    "        cells = row.find_all('td')\n",
    "        for cell in cells:\n",
    "            if count % 2 == 0:\n",
    "                main_trans_head.append(cell.text.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "                cont = cell.text.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "                if \"NOK\" in cont:\n",
    "                    cont = cont.split(\"NOK\")[0]\n",
    "                main_trans_cont.append(cont)\n",
    "            count += 1\n",
    "\n",
    "    results = soup.find('table', class_='table-striped')\n",
    "\n",
    "    header_elements = results.find_all(class_='cell-title')\n",
    "\n",
    "    headers = []\n",
    "    for elem in header_elements:\n",
    "        headers.append(elem.text.strip())\n",
    "\n",
    "    rows_all = results.find_all('tr', class_='ng-star-inserted')\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for rows_elem in rows_all:\n",
    "        row_cell = rows_elem.find_all('td', class_='ng-star-inserted')\n",
    "        row = []\n",
    "        for cell in row_cell:\n",
    "            row.append(cell.text.strip())\n",
    "        rows.append(row)\n",
    "\n",
    "    \n",
    "    output = []\n",
    "    output.append(main_trans_head)\n",
    "    output.append(main_trans_cont)\n",
    "    \n",
    "    output.append(headers)\n",
    "\n",
    "    for row in main_temp:\n",
    "        output.append(row)\n",
    "    output.append([])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for row in main_trs:\n",
    "    link_cell = row.find('div', class_='link')\n",
    "    if link_cell:\n",
    "        html_id = link_cell.get('id')\n",
    "        print(html_id)\n",
    "        driver.find_element_by_id(html_id).click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Run function on each page\n",
    "        output_temp = per_subpage(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "        output.append(output_temp)\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(3)\n",
    "        #break\n",
    "        for row in output_temp:\n",
    "            print(row)\n",
    "print(f'Done with page {url}! Add a different url at top of notebook to crawl something else.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{root}{output_csv}'\n",
    "with open(path, mode='w') as nets_file:\n",
    "    nets_writer = csv.writer(nets_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for page in output:\n",
    "        for row in page:\n",
    "            nets_writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
